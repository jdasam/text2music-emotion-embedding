{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Load a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from model import MyModel\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# load checkpoint\n",
    "DATA_PATH = './../../data/'\n",
    "DEVICE = 'cpu'\n",
    "S = torch.load(os.path.join(DATA_PATH, 'pretrained/alm_cross.ckpt'), map_location=torch.device(DEVICE))['state_dict']\n",
    "NS = {k[6:]: S[k] for k in S.keys() if (k[:5] == 'model')}\n",
    "\n",
    "# load model\n",
    "model = MyModel()\n",
    "model.load_state_dict(NS)\n",
    "model = model.eval()\n",
    "\n",
    "# load word2vec\n",
    "word2vec = pickle.load(open(os.path.join(DATA_PATH, 'w2v.pkl'), 'rb'))\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Map emotion tags to the shared embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['angry', 'scary', 'happy', 'sad', 'tender'] # add more emotions you like\n",
    "emotion_w2v = torch.tensor([word2vec[emotion] for emotion in emotions])\n",
    "with torch.no_grad():\n",
    "    emotion_embeddings = model.tag_to_embedding(emotion_w2v).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 64])\n"
     ]
    }
   ],
   "source": [
    "print(emotion_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Map text to the shared embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I am super happy today!'\n",
    "tokens = tokenizer([text, text], return_tensors='pt', padding=True, truncation=True) # made a list of the text to avoid batch_normalization issue\n",
    "with torch.no_grad():\n",
    "    text_embedding = model.text_to_embedding(tokens['input_ids'], tokens['attention_mask'])[0].detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(text_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Map music to the shared embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_LENGTH = 80000\n",
    "NUM_CHUNKS = 8\n",
    "get_spec = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=512, f_min=0.0, f_max=8000.0, n_mels=128)\n",
    "\n",
    "# load audio\n",
    "song = np.zeros((1, 16000 * 30)).astype('float32') # an example of 30-second of audio\n",
    "\n",
    "# get multiple chunks\n",
    "hop = (len(song) - INPUT_LENGTH) // NUM_CHUNKS\n",
    "song = torch.tensor([song[i*hop:i*hop+INPUT_LENGTH] for i in range(NUM_CHUNKS)]).squeeze(1)\n",
    "with torch.no_grad():\n",
    "    spec = get_spec(song)\n",
    "    song_embedding = model.spec_to_embedding(spec).detach().cpu().mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(song_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
